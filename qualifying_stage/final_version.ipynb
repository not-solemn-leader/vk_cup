{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_version.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "D7Vp3HhjZOP8",
        "3sd3Nv3ma7IA",
        "yS2o2nlhbFR5",
        "3BTWdwIzfqoX",
        "rZLS3_GrIzWe",
        "rnUaB1RbWv6p",
        "NMEiMv6_XvV5",
        "3zrfG2NdeHvR",
        "RyQeHVH4xjr2",
        "dupEJZ2rxrna",
        "W-0Fe2EcwSFV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Vp3HhjZOP8",
        "colab_type": "text"
      },
      "source": [
        "## Imports and boring stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Ucn4Wp74z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/vk_ads')\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWU6wL72oHWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install catboost\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebOi-_BdZ-Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "import seaborn as sns\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sd3Nv3ma7IA",
        "colab_type": "text"
      },
      "source": [
        "## Competition metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfa6AUWSa-MZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_smoothed_log_mape_column_value(responses_column, answers_column, epsilon):\n",
        "  return np.abs(np.log(\n",
        "      (responses_column + epsilon)\n",
        "      / (answers_column + epsilon)\n",
        "  )).mean()\n",
        "\n",
        "\n",
        "def get_smoothed_mean_log_accuracy_ratio(answers, responses, epsilon=0.005):\n",
        "  log_accuracy_ratio_mean = np.array(\n",
        "      [\n",
        "          get_smoothed_log_mape_column_value(responses.at_least_one, answers.at_least_one, epsilon),\n",
        "          get_smoothed_log_mape_column_value(responses.at_least_two, answers.at_least_two, epsilon),\n",
        "          get_smoothed_log_mape_column_value(responses.at_least_three, answers.at_least_three, epsilon),\n",
        "      ]\n",
        "  ).mean()\n",
        "\n",
        "  percentage_error = 100 * (np.exp(log_accuracy_ratio_mean) - 1)\n",
        "\n",
        "  return percentage_error.round(\n",
        "      decimals=2\n",
        "  )\n",
        "\n",
        "def cost(answers, responses, epsilon=0.005):\n",
        "  log_accuracy_ratio_mean = np.array(\n",
        "      [\n",
        "          get_smoothed_log_mape_column_value(responses[0], answers[0], epsilon),\n",
        "          get_smoothed_log_mape_column_value(responses[1], answers[1], epsilon),\n",
        "          get_smoothed_log_mape_column_value(responses[2], answers[2], epsilon),\n",
        "      ]\n",
        "  ).mean()\n",
        "\n",
        "  percentage_error = 100 * (np.exp(log_accuracy_ratio_mean) - 1)\n",
        "\n",
        "  return percentage_error.round(\n",
        "      decimals=2\n",
        "  )\n",
        "\n",
        "def cv_cost(est, X, y):\n",
        "    return cost(est.predict(X), y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS2o2nlhbFR5",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0kT5VyOZ6xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_tsv(fp):\n",
        "  return pd.read_csv(fp, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuyZ9_aHZ0Nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "his_df = load_tsv('data/history.tsv')\n",
        "us_df = load_tsv('data/users.tsv').astype('category')\n",
        "xval_df = load_tsv('data/validate.tsv')\n",
        "yval_df = load_tsv('data/validate_answers.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOe271zfVmP4",
        "colab_type": "text"
      },
      "source": [
        "## How we cross validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VswQ5PJI97Lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossval(model, xval_df, yval_df, train_sizes=[0.7, 0.75, 0.8]):\n",
        "    scores = []\n",
        "    for ts in train_sizes:              \n",
        "        mid_scores = []\n",
        "        data_sorted = pd.concat([xval_df, yval_df], axis=1).sort_values(by='hour_start')\n",
        "        x_cols, y_cols = xval_df.columns, yval_df.columns\n",
        "        n_train_samples = int(len(data_sorted) * ts)\n",
        "\n",
        "        train_df = data_sorted[x_cols].iloc[:n_train_samples]\n",
        "        y_train = te(data_sorted[y_cols].iloc[:n_train_samples].values)\n",
        "\n",
        "        test_df = data_sorted[x_cols].iloc[n_train_samples:]\n",
        "        y_test = te(data_sorted[y_cols].iloc[n_train_samples:].values)\n",
        "\n",
        "        av_his = his_df[his_df['hour'] < test_df['hour_start'].min()]\n",
        "\n",
        "        X_train, cat_features = fe(train_df, av_his)\n",
        "        X_test, _ = fe(test_df, av_his)\n",
        "        \n",
        "        model.fit(X_train, y_train)\n",
        "        clear_output()\n",
        "        mid_scores.append(cost(pe(y_test), pe(model.predict(X_test))))\n",
        "        scores.append(mid_scores)\n",
        "    print('\\n'.join([('%f percenst split: '+str(scores[i])) % train_sizes[i] for i in range(len(scores))]))\n",
        "    scores = np.array(scores)\n",
        "    return (scores.mean(axis=0), scores.std(axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcSLsIOXKu_O",
        "colab_type": "text"
      },
      "source": [
        "# Machine learning actually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w4qqsfwSlr_8",
        "colab": {}
      },
      "source": [
        "def percentile(n):\n",
        "    def percentile_(x):\n",
        "        return x.quantile(n)\n",
        "    percentile_.__name__ = 'percentile_%s' % n\n",
        "    return percentile_\n",
        "\n",
        "def get_ta(ad):  # return df of users\n",
        "    ids = [int(i) for i in ad['user_ids'].split(',')]\n",
        "    aus = ad['audience_size']\n",
        "    ta = us_df[us_df['user_id'].isin(ids)]  # target auditory\n",
        "    assert ta.shape[0] == aus\n",
        "    return ta\n",
        "\n",
        "def get_n_tcities(ta):  # return number of target cities\n",
        "    return sum([1 for i in ta['city_id'].value_counts().values if i != 0])\n",
        "\n",
        "def get_ages_mean_std(ta):  # return mean and std of ages distr without outliers\n",
        "    ta = ta.astype('int')\n",
        "    ages = ta[(ta['age'] >= 14) & (ta['age'] <= 80)]['age']\n",
        "    if ages.empty:\n",
        "        ages = np.array([0])\n",
        "    return [ages.mean(), ages.std()]\n",
        "\n",
        "def get_male_perc(ta):  # percentage of men\n",
        "    return ta['sex'].value_counts(normalize=True)[1] * 100\n",
        "\n",
        "def get_new_features(ad):\n",
        "    ta = get_ta(ad)\n",
        "    ta = ta.astype('int')\n",
        "    new_cols = []\n",
        "    new_cols.append(get_n_tcities(ta))\n",
        "    new_cols += get_ages_mean_std(ta)\n",
        "    new_cols.append(get_male_perc(ta))\n",
        "    return new_cols\n",
        "\n",
        "def users_hist_features(ad, hd_grouped):\n",
        "    # returns:\n",
        "    # 1. mean number of seen ads for target auditory\n",
        "    return pd.Series(hd_grouped.loc[ad.users].agg(['mean']).values.flatten(), index=['mean_ads_seen_per_user',])\n",
        "\n",
        "def pub_us_hist_features(ad, hist_grouped):\n",
        "    ta = [int(i) for i in ad['user_ids'].split(',')]\n",
        "    pubs = [int(i) for i in ad.publishers.split(',')]\n",
        "    ta_tp_history = hist_grouped[((hist_grouped['publisher'].isin(pubs)) & (hist_grouped['user_id'].isin(ta)))]\n",
        "    h = ta_tp_history.groupby('user_id').agg(['sum'])\n",
        "    h.columns = ['publisher_size', 'n_seen_ads_on_theese_platforms']\n",
        "    agg_funcs = ['median', 'mean', 'std', 'sum', percentile(0.25), percentile(0.75)]\n",
        "    x = h['n_seen_ads_on_theese_platforms'].agg(agg_funcs)\n",
        "    ### можно выбросить тех, кто ни разу не видел \n",
        "    x.index = [\n",
        "               'ta_tp_seen_ads_median', 'ta_tp_seen_ads_mean', 'ta_tp_seen_ads_std', 'ta_tp_seen_ads_sum', 'ta_tp_seen_ads_q1', 'ta_tp_seen_ads_q3'\n",
        "               ]\n",
        "    x['n_of_people_who_didnt_see'] = len(ta) - len(h)\n",
        "    x['n_of_people_who_saw_at_least_once'] = h['n_seen_ads_on_theese_platforms'].value_counts()[0:].sum()\n",
        "    x['n_of_people_who_saw_at_least_twice'] = h['n_seen_ads_on_theese_platforms'].value_counts()[1:].sum()\n",
        "    x['n_of_people_who_saw_at_least_three_times'] = h['n_seen_ads_on_theese_platforms'].value_counts()[2:].sum()\n",
        "    return x\n",
        "\n",
        "def fe(X, hist):  # feature engeneering, returns enged X, cat_features\n",
        "    X['users'] = [list(map(int, i.split(','))) for i in X['user_ids']] \n",
        "    X['time_shown'] = X['hour_end'] - X['hour_start']\n",
        "    hist['day_hour'] = hist['hour'] % 24\n",
        "    new_X = pd.DataFrame()\n",
        "\n",
        "    # basic ad features\n",
        "    new_X['cpm'] = X['cpm']\n",
        "    new_X['time_shown'] = X['time_shown']\n",
        "    new_X['audience_size'] = X['audience_size']\n",
        "\n",
        "    # user info features\n",
        "    ui_X = X.apply(get_new_features, axis=1, result_type='expand')\n",
        "    ui_cols = ['n_target_cities', 'tage_mean', 'tage_std', 'male_perc']\n",
        "    ui_X.columns = ui_cols\n",
        "    new_X = pd.concat([new_X, ui_X], axis=1)\n",
        "\n",
        "    # history features 1\n",
        "    hist_grouped = hist.groupby('user_id')[['cpm']].agg(['size'])\n",
        "    us_hist_X = X.apply(\n",
        "        users_hist_features, axis=1, result_type='expand', args=(hist_grouped,),\n",
        "    )\n",
        "    new_X = pd.concat([new_X, us_hist_X], axis=1)\n",
        "\n",
        "    # history features 2\n",
        "    hist_grouped = hist.groupby(['publisher', 'user_id'])['cpm'].agg(['size']).reset_index()\n",
        "    pub_us_hist_X = X.apply(\n",
        "        pub_us_hist_features, axis=1, result_type='expand', args=(hist_grouped,)\n",
        "    )\n",
        "    new_X = pd.concat([new_X, pub_us_hist_X], axis=1)\n",
        "\n",
        "    # define categorical features\n",
        "    cat_features = []\n",
        "    new_X[cat_features] = new_X[cat_features].astype('int')\n",
        "    poly = PolynomialFeatures(2)\n",
        "    new_X = poly.fit_transform(new_X)\n",
        "    return (new_X, cat_features)\n",
        "\n",
        "def pe(y):  # postprocess target\n",
        "    return y ** 2\n",
        "\n",
        "def te(y):  # target engeneering\n",
        "    return np.sqrt(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHtLVnjFkrrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "X, cat_features = fe(xval_df, his_df)\n",
        "y = te(yval_df.values) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbbWjBFRnEwe",
        "colab_type": "text"
      },
      "source": [
        "## Define base models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAnUccTalS8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCatboostRegressor(CatBoostRegressor):\n",
        "    def predict(self, data):\n",
        "        preds = super(MyCatboostRegressor, self).predict(data)\n",
        "        preds = np.maximum(preds, 0.)\n",
        "        preds = np.minimum(preds, 1.)\n",
        "        preds = np.round(preds, 4)\n",
        "        return preds\n",
        "\n",
        "\n",
        "class WorldGreatestModel(object):\n",
        "    # basically, simple ensemble\n",
        "    def __init__(self, estimators=None):\n",
        "        self.estimators = estimators\n",
        "    \n",
        "    def fit(self, X, y, n_folds=4):\n",
        "        for i in range(len(self.estimators)):\n",
        "            self.estimators[i].fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = [self.postprocess_y(self.estimators[i].predict(X)) for i in range(len(self.estimators))]\n",
        "        preds = sum(preds) / len(self.estimators)\n",
        "        return preds\n",
        "\n",
        "    def postprocess_y(self, preds):\n",
        "        preds = np.maximum(preds, 0.)\n",
        "        preds = np.minimum(preds, 1.)\n",
        "        preds = np.round(preds, 4)\n",
        "        return preds\n",
        "\n",
        "    def save_models(self, fps):\n",
        "        assert len(fps) == len(self.estimators)\n",
        "        for i in range(len(fps)):\n",
        "            self.estimators[i].save_model(fps[i], 'json')\n",
        "\n",
        "    def load_models(self, fps):\n",
        "        self.estimators = [MyCatboostRegressor().load_model(fp, 'json') for fp in fps]\n",
        "        return self\n",
        "\n",
        "n_iterations = 3000\n",
        "\n",
        "estimators = [\n",
        "    MyCatboostRegressor(\n",
        "      loss_function= 'MultiRMSE',\n",
        "      iterations= n_iterations,\n",
        "      random_seed=2,\n",
        "    ),\n",
        "    MyCatboostRegressor(\n",
        "      loss_function= 'MultiRMSE',\n",
        "      iterations=n_iterations,\n",
        "      random_seed=189,\n",
        "    ),\n",
        "    MyCatboostRegressor(\n",
        "      loss_function= 'MultiRMSE',\n",
        "      iterations=n_iterations,\n",
        "      random_seed=101,\n",
        "    ),\n",
        "    MyCatboostRegressor(\n",
        "      loss_function= 'MultiRMSE',\n",
        "      iterations=n_iterations,\n",
        "      random_seed=42,\n",
        "    ),\n",
        "]\n",
        "\n",
        "model = WorldGreatestModel(estimators)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uDwSDbeGwek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "    ensemble_size = 4\n",
        "    file_names = ['tuned_ensemble/' + 'model' + str(i) for i in range(ensemble_size)]\n",
        "    model.fit(X, y)\n",
        "    model.save_models(file_names) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYo2pInGlV2B",
        "colab_type": "text"
      },
      "source": [
        "## CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSOffQ4plde8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "if True:\n",
        "    kfold5 = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "    kfold9 = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
        "\n",
        "    scores_mean, scores_std = crossval(\n",
        "        model, xval_df, yval_df, train_sizes=kfold9 \n",
        "    )\n",
        "    print('Mean scores', scores_mean)\n",
        "    print('Std of scores', scores_std)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}